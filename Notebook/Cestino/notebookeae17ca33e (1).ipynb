{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T16:09:38.107418Z","iopub.status.busy":"2023-05-12T16:09:38.106778Z","iopub.status.idle":"2023-05-12T16:09:51.701926Z","shell.execute_reply":"2023-05-12T16:09:51.700980Z","shell.execute_reply.started":"2023-05-12T16:09:38.107389Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path_rgb</th>\n","      <th>path_depth</th>\n","      <th>path_mask</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","      <td>../SubDataset/scene/scan_00000/00000_00000_ind...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             path_rgb  \\\n","0   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","1   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","2   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","3   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","4   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","5   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","6   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","7   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","8   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","9   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","10  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","11  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","12  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","13  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","14  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","15  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","16  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","17  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","18  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","19  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","20  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","21  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","22  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","23  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","24  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","25  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","26  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","27  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","28  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","29  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","30  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","31  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","32  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","33  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","34  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","35  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","36  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","37  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","38  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","39  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","40  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","41  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","42  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","43  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","44  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","45  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","\n","                                           path_depth  \\\n","0   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","1   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","2   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","3   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","4   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","5   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","6   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","7   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","8   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","9   ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","10  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","11  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","12  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","13  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","14  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","15  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","16  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","17  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","18  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","19  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","20  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","21  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","22  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","23  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","24  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","25  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","26  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","27  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","28  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","29  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","30  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","31  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","32  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","33  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","34  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","35  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","36  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","37  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","38  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","39  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","40  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","41  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","42  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","43  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","44  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","45  ../SubDataset/scene/scan_00000/00000_00000_ind...   \n","\n","                                            path_mask  \n","0   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","1   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","2   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","3   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","4   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","5   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","6   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","7   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","8   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","9   ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","10  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","11  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","12  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","13  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","14  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","15  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","16  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","17  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","18  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","19  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","20  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","21  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","22  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","23  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","24  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","25  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","26  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","27  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","28  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","29  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","30  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","31  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","32  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","33  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","34  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","35  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","36  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","37  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","38  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","39  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","40  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","41  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","42  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","43  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","44  ../SubDataset/scene/scan_00000/00000_00000_ind...  \n","45  ../SubDataset/scene/scan_00000/00000_00000_ind...  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df=pd.read_csv(\"../Csv/TOY.csv\")\n","df.rename(columns={'path_rgb' :'image', 'path_mask': 'mask', 'path_depth':'depth'})\n","# elimina le prime tre colonne\n","df = df.iloc[:, 3:]\n","df"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T16:22:00.098740Z","iopub.status.busy":"2023-05-12T16:22:00.098018Z","iopub.status.idle":"2023-05-12T16:22:00.106128Z","shell.execute_reply":"2023-05-12T16:22:00.105186Z","shell.execute_reply.started":"2023-05-12T16:22:00.098702Z"},"trusted":true},"outputs":[],"source":["HEIGHT = 256\n","WIDTH = 256\n","LR = 0.001\n","EPOCHS = 15\n","BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T16:10:48.008395Z","iopub.status.busy":"2023-05-12T16:10:48.007971Z","iopub.status.idle":"2023-05-12T16:10:48.025428Z","shell.execute_reply":"2023-05-12T16:10:48.024438Z","shell.execute_reply.started":"2023-05-12T16:10:48.008364Z"},"trusted":true},"outputs":[],"source":["class DataGenerator(tf.keras.utils.Sequence):\n","    def __init__(self, data, batch_size=6, dim=(768, 1024), n_channels=3, shuffle=True):\n","        \"\"\"\n","        Initialization\n","        \"\"\"\n","        self.data = data\n","        self.indices = self.data.index.tolist()\n","        self.dim = dim\n","        self.n_channels = n_channels\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.min_depth = 0.1\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.data) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        if (index + 1) * self.batch_size > len(self.indices):\n","            self.batch_size = len(self.indices) - index * self.batch_size\n","        # Generate one batch of data\n","        # Generate indices of the batch\n","        index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n","        # Find list of IDs\n","        batch = [self.indices[k] for k in index]\n","        x, y = self.data_generation(batch)\n","\n","        return x, y\n","\n","    def on_epoch_end(self):\n","\n","        \"\"\"\n","        Updates indexes after each epoch\n","        \"\"\"\n","        self.index = np.arange(len(self.indices))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.index)\n","\n","    def load(self, image_path, depth_map, mask):\n","        \"\"\"Load input and target image.\"\"\"\n","\n","        image_ = cv2.imread(image_path)\n","        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n","        image_ = cv2.resize(image_, self.dim)\n","        image_ = tf.image.convert_image_dtype(image_, tf.float32)\n","\n","        depth_map = np.load(depth_map).squeeze()\n","\n","        mask = np.load(mask)\n","        mask = mask > 0\n","\n","        max_depth = min(300, np.percentile(depth_map, 99))\n","        depth_map = np.clip(depth_map, self.min_depth, max_depth)\n","        depth_map = np.log(depth_map, where=mask)\n","\n","        depth_map = np.ma.masked_where(~mask, depth_map)\n","\n","        depth_map = np.clip(depth_map, 0.1, np.log(max_depth))\n","        depth_map = cv2.resize(depth_map, self.dim)\n","        depth_map = np.expand_dims(depth_map, axis=2)\n","        depth_map = tf.image.convert_image_dtype(depth_map, tf.float32)\n","\n","        return image_, depth_map\n","\n","    def data_generation(self, batch):\n","\n","        x = np.empty((self.batch_size, *self.dim, self.n_channels))\n","        y = np.empty((self.batch_size, *self.dim, 1))\n","\n","        for i, batch_id in enumerate(batch):\n","            x[i,], y[i,] = self.load(\n","                self.data[\"image\"][batch_id],\n","                self.data[\"depth\"][batch_id],\n","                self.data[\"mask\"][batch_id],\n","            )\n","\n","        return x, y"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T16:10:51.798107Z","iopub.status.busy":"2023-05-12T16:10:51.797722Z","iopub.status.idle":"2023-05-12T16:10:51.817514Z","shell.execute_reply":"2023-05-12T16:10:51.816658Z","shell.execute_reply.started":"2023-05-12T16:10:51.798060Z"},"trusted":true},"outputs":[],"source":["class DownscaleBlock(layers.Layer):\n","    def __init__(\n","        self, filters, kernel_size=(3, 3), padding=\"same\", strides=1, **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n","        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n","        self.reluA = layers.LeakyReLU(alpha=0.2)\n","        self.reluB = layers.LeakyReLU(alpha=0.2)\n","        self.bn2a = tf.keras.layers.BatchNormalization()\n","        self.bn2b = tf.keras.layers.BatchNormalization()\n","\n","        self.pool = layers.MaxPool2D((2, 2), (2, 2))\n","\n","    def call(self, input_tensor):\n","        d = self.convA(input_tensor)\n","        x = self.bn2a(d)\n","        x = self.reluA(x)\n","\n","        x = self.convB(x)\n","        x = self.bn2b(x)\n","        x = self.reluB(x)\n","\n","        x += d\n","        p = self.pool(x)\n","        return x, p\n","\n","\n","class UpscaleBlock(layers.Layer):\n","    def __init__(\n","        self, filters, kernel_size=(3, 3), padding=\"same\", strides=1, **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","        self.us = layers.UpSampling2D((2, 2))\n","        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n","        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n","        self.reluA = layers.LeakyReLU(alpha=0.2)\n","        self.reluB = layers.LeakyReLU(alpha=0.2)\n","        self.bn2a = tf.keras.layers.BatchNormalization()\n","        self.bn2b = tf.keras.layers.BatchNormalization()\n","        self.conc = layers.Concatenate()\n","\n","    def call(self, x, skip):\n","        x = self.us(x)\n","        concat = self.conc([x, skip])\n","        x = self.convA(concat)\n","        x = self.bn2a(x)\n","        x = self.reluA(x)\n","\n","        x = self.convB(x)\n","        x = self.bn2b(x)\n","        x = self.reluB(x)\n","\n","        return x\n","\n","\n","class BottleNeckBlock(layers.Layer):\n","    def __init__(\n","        self, filters, kernel_size=(3, 3), padding=\"same\", strides=1, **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n","        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n","        self.reluA = layers.LeakyReLU(alpha=0.2)\n","        self.reluB = layers.LeakyReLU(alpha=0.2)\n","\n","    def call(self, x):\n","        x = self.convA(x)\n","        x = self.reluA(x)\n","        x = self.convB(x)\n","        x = self.reluB(x)\n","        return x"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T16:10:55.711085Z","iopub.status.busy":"2023-05-12T16:10:55.710705Z","iopub.status.idle":"2023-05-12T16:10:55.730418Z","shell.execute_reply":"2023-05-12T16:10:55.729454Z","shell.execute_reply.started":"2023-05-12T16:10:55.711037Z"},"trusted":true},"outputs":[],"source":["class DepthEstimationModel(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.ssim_loss_weight = 0.85\n","        self.l1_loss_weight = 0.1\n","        self.edge_loss_weight = 0.9\n","        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n","        f = [16, 32, 64, 128, 256]\n","        self.downscale_blocks = [\n","            DownscaleBlock(f[0]),\n","            DownscaleBlock(f[1]),\n","            DownscaleBlock(f[2]),\n","            DownscaleBlock(f[3]),\n","        ]\n","        self.bottle_neck_block = BottleNeckBlock(f[4])\n","        self.upscale_blocks = [\n","            UpscaleBlock(f[3]),\n","            UpscaleBlock(f[2]),\n","            UpscaleBlock(f[1]),\n","            UpscaleBlock(f[0]),\n","        ]\n","        self.conv_layer = layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"tanh\")\n","\n","    def calculate_loss(self, target, pred):\n","        # Edges\n","        dy_true, dx_true = tf.image.image_gradients(target)\n","        dy_pred, dx_pred = tf.image.image_gradients(pred)\n","        weights_x = tf.exp(tf.reduce_mean(tf.abs(dx_true)))\n","        weights_y = tf.exp(tf.reduce_mean(tf.abs(dy_true)))\n","\n","        # Depth smoothness\n","        smoothness_x = dx_pred * weights_x\n","        smoothness_y = dy_pred * weights_y\n","\n","        depth_smoothness_loss = tf.reduce_mean(abs(smoothness_x)) + tf.reduce_mean(\n","            abs(smoothness_y)\n","        )\n","\n","        # Structural similarity (SSIM) index\n","        ssim_loss = tf.reduce_mean(\n","            1\n","            - tf.image.ssim(\n","                target, pred, max_val=WIDTH, filter_size=7, k1=0.01 ** 2, k2=0.03 ** 2\n","            )\n","        )\n","        # Point-wise depth\n","        l1_loss = tf.reduce_mean(tf.abs(target - pred))\n","\n","        loss = (\n","            (self.ssim_loss_weight * ssim_loss)\n","            + (self.l1_loss_weight * l1_loss)\n","            + (self.edge_loss_weight * depth_smoothness_loss)\n","        )\n","\n","        return loss\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric]\n","\n","    def train_step(self, batch_data):\n","        input, target = batch_data\n","        with tf.GradientTape() as tape:\n","            pred = self(input, training=True)\n","            loss = self.calculate_loss(target, pred)\n","\n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","        self.loss_metric.update_state(loss)\n","        return {\n","            \"loss\": self.loss_metric.result(),\n","        }\n","\n","    def test_step(self, batch_data):\n","        input, target = batch_data\n","\n","        pred = self(input, training=False)\n","        loss = self.calculate_loss(target, pred)\n","\n","        self.loss_metric.update_state(loss)\n","        return {\n","            \"loss\": self.loss_metric.result(),\n","        }\n","\n","    def call(self, x):\n","        c1, p1 = self.downscale_blocks[0](x)\n","        c2, p2 = self.downscale_blocks[1](p1)\n","        c3, p3 = self.downscale_blocks[2](p2)\n","        c4, p4 = self.downscale_blocks[3](p3)\n","\n","        bn = self.bottle_neck_block(p4)\n","\n","        u1 = self.upscale_blocks[0](bn, c4)\n","        u2 = self.upscale_blocks[1](u1, c3)\n","        u3 = self.upscale_blocks[2](u2, c2)\n","        u4 = self.upscale_blocks[3](u3, c1)\n","\n","        return self.conv_layer(u4)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T16:22:07.873467Z","iopub.status.busy":"2023-05-12T16:22:07.873099Z","iopub.status.idle":"2023-05-12T16:34:32.349938Z","shell.execute_reply":"2023-05-12T16:34:32.329023Z","shell.execute_reply.started":"2023-05-12T16:22:07.873440Z"},"trusted":true},"outputs":[{"ename":"KeyError","evalue":"'image'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m train_loader \u001b[39m=\u001b[39m DataGenerator(\n\u001b[0;32m     14\u001b[0m     data\u001b[39m=\u001b[39mdf[:\u001b[39m20\u001b[39m]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m), batch_size\u001b[39m=\u001b[39mBATCH_SIZE, dim\u001b[39m=\u001b[39m(HEIGHT, WIDTH)\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m validation_loader \u001b[39m=\u001b[39m DataGenerator(\n\u001b[0;32m     17\u001b[0m     data\u001b[39m=\u001b[39mdf[\u001b[39m20\u001b[39m:]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m), batch_size\u001b[39m=\u001b[39mBATCH_SIZE, dim\u001b[39m=\u001b[39m(HEIGHT, WIDTH)\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     20\u001b[0m     train_loader,\n\u001b[0;32m     21\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[0;32m     22\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_loader,\n\u001b[0;32m     23\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","Cell \u001b[1;32mIn[16], line 26\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Find list of IDs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m batch \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m index]\n\u001b[1;32m---> 26\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_generation(batch)\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m x, y\n","Cell \u001b[1;32mIn[16], line 72\u001b[0m, in \u001b[0;36mDataGenerator.data_generation\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     68\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim, \u001b[39m1\u001b[39m))\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m i, batch_id \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch):\n\u001b[0;32m     71\u001b[0m     x[i,], y[i,] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(\n\u001b[1;32m---> 72\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m][batch_id],\n\u001b[0;32m     73\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mdepth\u001b[39m\u001b[39m\"\u001b[39m][batch_id],\n\u001b[0;32m     74\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m][batch_id],\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m x, y\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 'image'"]}],"source":["optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=LR,\n","    amsgrad=False,\n",")\n","model = DepthEstimationModel()\n","# Define the loss function\n","cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction=\"none\"\n",")\n","# Compile the model\n","model.compile(optimizer, loss=cross_entropy)\n","\n","train_loader = DataGenerator(\n","    data=df[:20].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH)\n",")\n","validation_loader = DataGenerator(\n","    data=df[20:].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH)\n",")\n","model.fit(\n","    train_loader,\n","    epochs=EPOCHS,\n","    validation_data=validation_loader,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
